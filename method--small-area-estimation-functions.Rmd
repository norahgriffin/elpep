<!-- # Develop Methods for Small Area Estimation ------------------------------>

<!-- ## Set functions direct estimation --------------------------------------->

```{r function to run direct estimation}
direct_est_tract <- function(data,          # Individual-level sample data used to calculate the measure of interest--
                                            #   specifically *shares* rather than counts--at higher geographic levels
                             aux_data,      # Auxiliary data, used to apply shares to the tract level
                             measure,       # The binary indicator used to calculate share values
                             vul_cat_value, # Character label for the shares measure
                             n_pt_variable  # Character name for the tract-level count variable in the aux data
                             ) {
  
  # Use the `emdi::direct()` function to calculate PUMA-level stats and standard
  # errors from the sample data
  direct_est_puma <- 
    emdi::direct(y = measure, 
           smp_data = data %>% dplyr::select(one_of(measure, "PUMA", "PWGTP")), 
             # We subset the data here because the na.rm = TRUE argument in this
             # function call removes records with *any* missing values, 
             # regardless of whether they are the ones used directly in the run.
           smp_domains = "PUMA", 
           weights = "PWGTP", 
           var = TRUE, 
           threshold = NULL,
           boot_type = "naive", 
           B = 50, 
           seed = 123, 
           na.rm = TRUE)
  
  ### Compare with hand-rolled calculation
  # /!\ Note: am planning to bootstrap standard errors given that our calculations
  # are using the weighted.mean 
  own_calc <-
    data[j = weighted.mean(x = get(measure), 
                           w = PWGTP, 
                           na.rm = TRUE), 
         by = PUMA] %>% 
    .[order(PUMA)]
  comp <- 
    merge(own_calc %>% mutate(PUMA = as.character(PUMA)), 
          direct_est_puma$ind %>% mutate(Domain = as.character(Domain)), 
          by.x = "PUMA", 
          by.y = "Domain")
  
  direct_out <- 
    merge(direct_est_puma$ind %>% dplyr::select(PUMA = Domain, share     = Mean),
          direct_est_puma$MSE %>% dplyr::select(PUMA = Domain, share_mse = Mean), 
          by = "PUMA") %>% 
    mutate(vc_value = vul_cat_value) %>% 
    data.table()
  
  return(direct_out)
}

```

<!-- ## Set functions for Fay-Herriot Method ---------------------------------->

```{r function the sae method one time}
run_sae <- function(direct_ests,                 # Direct, tract-level estimates as generated by the `direct_est_tracts()` function above
                    aux_data,                    # Auxiliary data at the tract level used to develop model-based estimates
                    aux_controls,                # Character vector of controls used in regression models
                    aux_n,                       # Count of individuals in the population of interest, used to convert share 
                                                 #   estimates of program eligibles to counts of eligibles
                    aux_n_se,                    # Std error of the `aux_n` measure
                    run_model_selection = FALSE, # Binary flag for whether to run model selection using the `emdi::fh()` function
                    B_spec = c(50, 0)            # Bootstrap specification for the `emdi::fh()` function
                    ) {
  
  # Method notes --------------------------------------------------------------#
  
  # This method is implemented differently than conventional SAE. Whereas other
  # implementations run and predict model results at the same level for which
  # direct estimates are calculated, we have direct results (shares) that are
  # measured at the PUMA-level, and a desire to get to the tract level. These
  # are made by conformable by assuming that the PUMA-level shares can be
  # assumed to be representative of corresponding tracts. This is an assumption,
  # but arguably a natural one. What is challenging is knowing how to handle
  # standard errors. PUMA-level shares have standard errors, which can be
  # adjusted upward to reflect the ratio between the tract and PUMA, to reflect
  # the greater level of noise at the smaller level. However, the modeling step
  # is more complicated, since the regression happens with Ns at the tract level
  # where I'd hoped that the provision of inflated variance would yield sensible
  # results. 
  #   However, the model estimates are extremely precise--which is
  # unexpected--and the majority of the EBLUP blending weight is going to
  # the model results. When I'd used the `sandbox--emdi_package_functionality.Rmd`
  # script to test out a sample exercise of increasing variance of the direct
  # estimates, I found that the output showed both a greater MSE for the direct
  # estimates (naturally), but also a *substantially* decreased model MSE, which
  # still surprises me. At least one thing that freshly strikes me as unnatural 
  # about my approach of inflating standard errors, especially in a Bayesian
  # framework, is that the variance of the direct estimates themselves is constant,
  # but I'm increasing the sampling noise, which would lead a Bayesian model to
  # think that their differences are not very reliable, and would reward the 
  # fixed component of a model that can somehow predict that variation, which it
  # infers is primarily noise.
  #   For the time being, I am going accept a very temporary solution: *not* 
  # adjusting the variance of the PUMA shares to not enter this unnatural bayesian
  # situation, I will apply them to tract-level model estimates because I must
  # in order to get estimates at the level I need to proceed, and in the
  # end, I will inflate the variance of the final estimate. 
  # In the future, I will (1) consider bootstrapping all relevant components in
  # the model step, including draws of the PUMA-level outcome, and linkage to
  # tracts where I draw values of the predictors from their known sample 
  # distribution; and (2) write the package authors to seek input on this application
  # because it wouldn't hurt.
  # See this GitHub issue for relevant discussion:
  #   https://github.com/nsmader/estimating-support-eligible-population-counts/issues/6
  
  
  # Prepare input data --------------------------------------------------------#
  
  # Copy and subset the auxiliary data used for modeling
  #browser()
  aux_data_use <- 
    # Note: because we use data.table functions for calculating fields, we 
    # use `copy()` to avoid adjusting the original data with by-reference operations
    copy(aux_data) %>% 
    merge(geo_crosswalk %>% dplyr::select(GEOID, PUMA) %>% unique(), 
          by = "GEOID") %>%
    .[PUMA %in% direct_ests$PUMA,
      c("GEOID", "PUMA", aux_controls, aux_n, aux_n_se),
      with = FALSE] %>% 
    setnames(old = c(aux_n,   aux_n_se), 
             new = c("aux_n", "aux_n_se")) %>% 
    # Calculate PUMA-level sums for adjusting standard errors at the end
    .[j = n_pt := aux_n] %>% 
    .[j = n_p  := sum(n_pt), by = PUMA]
  
  aux_n <- nrow(aux_data_use)
  aux_data_use <- na.omit(aux_data_use)
  aux_n_omit <- nrow(aux_data_use)
  if (aux_n_omit != aux_n) print(glue("{aux_n - aux_n_omit} rows were removed due to missings in the ACS5 data"))
  
    
  # Get the name of the vulnerability field directly from the direct estimates data
  vc_value <- direct_ests$vc_value[1]
  
  # Subset the relevant direct estimates data
  direct_ests_tract <- 
    direct_ests %>% 
    merge(geo_crosswalk %>% dplyr::select(PUMA, GEOID) %>% unique(), by = "PUMA") %>% 
    # Also merge to aux_data_use to ensure alignment of represented GEOIDs
    merge(aux_data_use, by = c("PUMA", "GEOID")) %>% 
    select(PUMA, GEOID, share, share_mse) 
    
  # Build combined data as required by the `emdi` package
  combined_data_tract <- 
    combine_data(
      pop_data    = aux_data_use,
      pop_domains = "GEOID",
      smp_data    = direct_ests_tract,
      smp_domains = "GEOID")
  
  if (FALSE) {
    # Also creating an alternative to examine what PUMA-level estimation would
    # look like
    combined_data_puma <- 
      combine_data(
        pop_data    = acs5puma,
        pop_domains = "PUMA",
        smp_data    = direct_ests,
        smp_domains = "PUMA") 
  }
  
  # Run Fay-Herriot method ----------------------------------------------------#
  
  # Build formula for model predictions
  fm <- 
    paste0("share ~ ", 
           paste(aux_controls, collapse = " + ")) %>% 
    as.formula()

  # Optionally run model selection
  if (run_model_selection) {
    # Run Fay-Herriot model
    fh_std <- fh(fixed = fm,
                 vardir = "share_mse", 
                 combined_data = combined_data,
                 domains = "GEOID", 
                 method = "ml", 
                 B = B_spec) # c(0, B)  
    select_model <- step(fh_std, criteria = "KICb2")
    selected_controls <- coefficients(select_model) %>% names() %>% setdiff("(Intercept)")
    selected_fm <- build_fm("share", selected_controls)
  } else {
    selected_fm <- fm
  }
  
  #browser()
  fh_tract <- fh(fixed = selected_fm,
                 vardir = "share_mse", 
                 combined_data = combined_data_tract,
                 domains = "GEOID", 
                 method = "ml", 
                 MSE = TRUE,
                 B = B_spec) # c(0, B)
  
  if (FALSE) {
   fh_puma <- fh(fixed = selected_fm,
                vardir = "share_mse", 
                combined_data = combined_data_puma,
                domains = "PUMA", 
                method = "ml", 
                MSE = TRUE,
                B = B_spec) # c(0, B) 
  }
    
  # Collect output ------------------------------------------------------------#
  
  fh_collected_tract <- 
    with(fh_tract, {
      data.frame(GEOID                  = ind$Domain,
                 share_direct           = ind$Direct,
                 share_direct_mse_unadj = MSE$Direct,
                 share_model            = model$fitted,
                 share_model_mse_unadj  = model$variance,
                 share_fh               = ind$FH,
                 share_fh_mse_unadj     = MSE$FH)
    }) %>% 
    # Check understanding of weights construction in fh() -- this checks out
    mutate(implied_wgt = (share_fh - share_model)/(share_direct - share_model),
           var_ratio_mod = share_direct_mse_unadj / (share_direct_mse_unadj + share_model_mse_unadj),
           var_ratio_dir = share_model_mse_unadj  / (share_direct_mse_unadj + share_model_mse_unadj),
           my_share_fh_mse_unadj1 = var_ratio_dir^2*share_direct_mse_unadj + (1-var_ratio_dir)^2*share_model_mse_unadj,
           my_share_fh_mse_unadj2 =   implied_wgt^2*share_direct_mse_unadj +   (1-implied_wgt)^2*share_model_mse_unadj,
           diff = share_fh_mse_unadj - my_share_fh_mse_unadj2) %>% 
    data.table()
  
  if (FALSE) {
    fh_collected_puma <- 
    with(fh_puma, {
      data.frame(PUMA                   = ind$Domain,
                 share_direct           = ind$Direct,
                 share_direct_mse_unadj = MSE$Direct,
                 share_model            = model$fitted,
                 share_model_mse_unadj  = model$variance,
                 share_fh               = ind$FH,
                 share_fh_mse_unadj     = MSE$FH)
    }) %>% 
    # Check understanding of weights construction in fh() -- this checks out, as
    # a linear combination of standard errors (not variances)
    mutate(implied_wgt = (share_fh - share_model)/(share_direct - share_model),
           var_ratio_mod = share_direct_mse_puma / (share_direct_mse_puma + share_model_mse_puma),
           var_ratio_dir = share_model_mse_puma  / (share_direct_mse_puma + share_model_mse_puma),
           my_share_fh_mse_puma1 = var_ratio_dir^2*share_direct_mse_puma + (1-var_ratio_dir)^2*share_model_mse_puma,
           my_share_fh_mse_puma5 =   implied_wgt^2*share_direct_mse_puma +   (1-implied_wgt)^2*share_model_mse_puma,
           diff = share_fh_mse_puma - my_share_fh_mse_puma5) %>% 
    data.table()
  }
  
  # /!\ Attempt alternative model estimation ----------------------------------#
  
  # Building my own estimation method to examine how closely hand-rolled estimates
  # would compare, towards the idea of doing something like LASSO, random forest,
  # or bootstrapping
  # Note that this method is frequentist, whereas `fh()` implements Bayesian EBLUP 
  # estimation
  
  if (FALSE) {
    my_fh_data_tract <- 
      direct_ests_tract %>% 
      merge(aux_data_use, by = c("PUMA", "GEOID")) %>% 
      data.table()
    my_fh_reg_tract <- 
      lm(as.formula(selected_fm),
         data = my_fh_data_tract)
    prediction_tract <- predict(my_fh_reg_tract, 
                                se.fit = TRUE)
    
    my_fh_data_tract %>% 
      .[j = `:=`(my_share_model     = prediction_tract$fit,
                 my_share_model_se  = prediction_tract$se.fit,
                 my_share_model_mse = prediction_tract$se.fit^2)]
    #View(cbind(fh_collected, prediction$fit, prediction$se.fit))
    comp <- merge(fh_collected_tract, 
                  my_fh_data_tract %>% dplyr::select(GEOID, my_share_model, my_share_model_se, my_share_model_mse), 
                  by = c("GEOID"))
    
    my_fh_data_puma <- 
      direct_ests %>% 
      merge(acs5puma, by = c("PUMA")) %>% 
      data.table()
    my_fh_reg_puma <- 
      lm(as.formula(selected_fm),
         data = my_fh_data_puma)
    prediction_puma <- predict(my_fh_reg_puma, 
                               se.fit = TRUE)
    
    comp_all_betas <- 
      merge(tidy(my_fh_reg_tract) %>% transmute(term          = term, 
                                                own_est_tract = round(estimate, 3),
                                                own_p_tract   = round(p.value, 3)),
            tidy(my_fh_reg_puma)  %>% transmute(term          = term, 
                                                own_est_puma  = round(estimate, 3), 
                                                own_p_puma    = round(p.value, 3)), by = "term") %>% 
      merge(fh_tract$model$coefficients %>% transmute(term         = rownames(.), 
                                                      fh_est_tract = round(coefficients, 3), 
                                                      fh_se_tract  = round(p.value, 3)),
            by = "term") %>% 
      merge(fh_puma$model$coefficients  %>% transmute(term         = rownames(.), 
                                                      fh_est_puma  = round(coefficients, 3), 
                                                      fh_se_puma   = round(p.value, 3)),
            by = "term") %>% 
      setcolorder(c("term", "own_est_tract", "own_est_puma", "fh_est_tract", "fh_est_puma"))
    
    View(comp_all_betas)
      # The fh() vs "own" coefficients estimates aren't terribly off: they're
      # within my rough expectations of how frequentist and bayesian methods would differ:
      # within "model-sampling" variation. However, the tract vs puma-level estimats
      # estimates are *way* off. 
      # - why are the tract estimates somehow more respectable? Is acs5puma constructed properly? 
  }
  
  
  # Inflate standard errors, reflecting tract-level estimates -----------------#
  
  # Check for tracts with zero population
  zero_count_tracts <- aux_data_use[n_pt == 0]$GEOID
  #if (FALSE) {
  if (length(zero_count_tracts) > 0) {
    print(paste0("Tracts with 0 population are: ", 
                 paste(zero_count_tracts, collapse = ", ")))
  }
  
  # We need to subset away from zero population tracts (or find some other way to
  # incorporate them)
  fh_inflated <- 
    fh_collected_tract %>% 
    merge(aux_data_use[n_pt != 0],
          by = "GEOID",
          all.x = TRUE) %>% 
    .[j = `:=`(share_direct_se = (n_p/n_pt)*sqrt(share_direct_mse_unadj),
               share_model_se  = (n_p/n_pt)*sqrt(share_model_mse_unadj),
               share_fh_se     = (n_p/n_pt)*sqrt(share_fh_mse_unadj))] %>% 
    # Note: `cv` is "coefficient of variation"
    .[j = `:=`(cv_direct = share_direct_se^2 / share_direct,
               cv_model  = share_model_se^2  / share_model,
               cv_fh     = share_fh_se^2     / share_fh)]
  
  # Produce counts and final estimates
  fh_counts <-
    fh_inflated %>% 
    mutate(vc_count    = share_fh*aux_n,
           vc_count_se = se_product(aux_n, share_fh, aux_n_se, share_fh_se),
           vc_value    = vc_value) %>% 
    data.table()
    
  return(list(counts = fh_counts,
              coeffs = coefficients(fh_tract))) 
}
```

```{r build function to aggregate and compare counts}
# The `compare_sae_agg()` function validates predictions of our SAE method against
# observed data, by aggregating to geographic levels at which those measures are 
# observed, and building comparison statistics and plots
compare_sae_agg <- function(sae_in,        # SAE estimates output by the `run_sae()` function
                            sae_count_var, # Character name of the field with SAE-estimated count data
                            agg_data,      # Observed aggregate data used for comparison
                            sae_vul_cat,   # Character vector name of field with vulnerability categories in SAE data
                            agg_vul_cat    # Character vector name of filed with vulnerability categories in agg data
                            ) {
  
  #browser()
  sae_puma <-
    sae_in %>% 
    .[j = .(sae_count_puma = sum(get(sae_count_var))),
      by = .(PUMA, 
             vulnerability_cat = get(sae_vul_cat))]
  
  obs_puma <- 
    agg_data %>% 
    .[j = .(obs_count_puma = sum(PWGTP)),
      by = .(PUMA, 
             vulnerability_cat = get(agg_vul_cat))]
  
  comp_sae_obs_puma <- 
    merge(sae_puma,
          obs_puma,
          by = c("PUMA", "vulnerability_cat"))
  
  # Calculate correlation statistic between SAE and agg counts
  comp_cor <- 
    comp_sae_obs_puma[j = cor(sae_count_puma, 
                              obs_count_puma),
                      by = vulnerability_cat]
  
  # Create a scatter plot comparison between SAE and agg values
  agg_comp_plot <- 
    ggplot(comp_sae_obs_puma,
           aes(x = obs_count_puma,
               y = sae_count_puma,
               group = PUMA)) +
    geom_point() + 
    geom_abline(intercept = 0,
                slope = 1) +
    scale_x_continuous(labels = comma) +
    scale_y_continuous(labels = comma) +
    facet_wrap(~ vulnerability_cat) +
    labs(title = "PUMA-Level Comparison of SAE Estimates with Observed Estimates",
         x = "ACS 1-Year",
         y = "SAE Estimates") +
    theme_minimal() +
    myTheme +
    theme(strip.background = element_rect(fill = "lightgray", 
                                          color = NA))
  
  out <- list(agg_comp_data = comp_sae_obs_puma,
              agg_comp_cor  = comp_cor,
              agg_comp_plot = agg_comp_plot)
  
  return(out)
  
}
```


```{r run and gather combinations SAE results for all combinations}
# The `run_sae_combos` function runs a loop of the `run_sae()` function across 
# a series of vulnerability groups
run_sae_combos <- function(vul_vars,        # character vector of potentially multiple categorical fields that define vulnerability groups
                           sae_input_data,  # the data with the PUMA-level sample data used in the direct estimation method
                           sae_controls,    # controls used in the modeling component of the SAE
                           aux_data,        # auxiliary data set, with tract-level predictors and count variables
                           aux_n_var,       # character name for variable in the aux data set for N of the smaller areas
                           aux_n_se_var,    # character name variable in the aux data set with std err of the N
                           verbose = FALSE, # report progress within the loop
                           viz_comp = TRUE  # generate plot of SAE versus direct estimates
                           ) {
  
  # Develop a single dimension to run through. This is necessary if multiple vulnerability
  # variables are use to build combinations of vulnerability status, e.g. "poverty level"
  # and "head of household employment in 'vulnerable' industry"
  #browser()
  sae_data <- 
    copy(sae_input_data) %>% 
    .[j = vulnerability_cat := apply(.SD, 1, paste, collapse = "__"),
      .SDcols = vul_vars]
  
  # Remove invalid combinations from runlist
  vcs <- sae_data$vulnerability_cat %>% unique() %>% sort() %>% setdiff("NA")  
  for (vc in vcs) {
    if (verbose) print(paste("Running direct estimation for vulnerability category", vc))
    # Prep data
    sae_data_vc <- 
      sae_data[j = is_vc := 1*(vulnerability_cat == vc)]
    
    # Run direct estimation
    direct_vc <-
      direct_est_tract(data          = sae_data_vc, 
                       aux_data      = aux_data,
                       measure       = "is_vc",
                       vul_cat_value = vc,
                       n_pt_variable = aux_n_var)
    
    # Run SAE
    sae_vc <- 
      run_sae(direct_ests  = direct_vc, 
              aux_data     = aux_data,
              aux_controls = sae_controls,
              aux_n        = aux_n_var,
              aux_n_se     = aux_n_se_var,
              B_spec       = c(100, 0))
    
    # Stack output
    coeffs_vc_add <- 
      data.frame(vc = vc,
                 terms = names(sae_vc[["coeffs"]]),
                 coeffs = sae_vc[["coeffs"]])
    if (vc == vcs[1]) {
      direct_vc_out <- direct_vc
      sae_vc_out <- sae_vc[["counts"]]
      coeffs_vc_out <- coeffs_vc_add
      
    } else {
      direct_vc_out <- bind_rows(direct_vc_out, direct_vc)
      sae_vc_out    <- bind_rows(sae_vc_out,    sae_vc[["counts"]])
      coeffs_vc_out <- bind_rows(coeffs_vc_out, coeffs_vc_add)
    }
  }
  #browser()
  ### Set a floor of 0 for share estimates and adjust other share estimates to 
  # ensure adding-up
  # /!\ Note that we do not currently set a ceiling of 100% as we have not 
  # empirically seen that yet, and generally expect it to be less likely.
  # We handle this by issuing a warning if encountered, as a signal that some
  # new, and different, handling is necessary.
  #browser()
  sae_vc_out <- 
    sae_vc_out %>% 
    # /!\ A very unexpected phenomenon is that data.table `by` calculations were
    # not recognizing the same value of GEOID as a group (instead, it was making
    # each element as a group). Simply reassigning the value seems to deal with
    # this. However, it's worth checking back to understand the origin, and see
    # if something about package version or data conditions change to prevent this.
    .[j = GEOID := as.character(GEOID)] %>% 
    .[j = `:=`(model_trunc_flag = share_model < 0,
                  fh_trunc_flag = share_fh    < 0,
               share_model_trunc = pmax(0, share_model),
                  share_fh_trunc = pmax(0, share_fh))] %>%
    .[j = `:=`(trunc_model_share_sum_t = sum(share_model_trunc),
                  trunc_fh_share_sum_t = sum(share_fh_trunc)),
      by = GEOID] %>% 
    
    .[model_trunc_flag == TRUE, share_model_se_trunc := 0] %>%
    .[fh_trunc_flag    == TRUE, share_fh_se_trunc    := 0] %>%
    
    .[model_trunc_flag == FALSE, 
      `:=`(share_model_trunc    = share_model    / trunc_model_share_sum_t, 
           share_model_se_trunc = share_model_se / trunc_model_share_sum_t)] %>% 
    .[fh_trunc_flag == FALSE,
      `:=`(share_fh_trunc       = share_fh       / trunc_fh_share_sum_t, 
           share_fh_se_trunc    = share_fh_se    / trunc_fh_share_sum_t)] %>% 
    
    .[j = `:=`(vc_model_count_trunc    = aux_n*share_model_trunc,
               vc_model_count_se_trunc = se_product(aux_n, share_model_trunc, aux_n_se, share_model_se_trunc),
               vc_fh_count_trunc       = aux_n*share_fh_trunc,
               vc_fh_count_se_trunc    = se_product(aux_n, share_fh_trunc,    aux_n_se, share_fh_se_trunc))]
  if (any(sae_vc_out$share_fh > 1)) {
    warning(glue("Encountered an estimated share above 100%.\n",
                 "This signals a need for a fix to be established."))
  }
    
  # Quick inspection of results
  if (FALSE) {
    summary(sae_vc_out)
    sae_vc_out[, cor(vc_model_count, vc_model_count_trunc)]
    sae_vc_out[, cor(   vc_fh_count,    vc_fh_count_trunc)]
    ggplot(sae_vc_out,
           aes(x = vc_fh_count,
               y = vc_fh_count_trunc,
               color = factor(vc_value))) +
      geom_point() +
      geom_abline(yintercept = 0,
                  slope = 1)
    sum(sae_vc_out$vc_count_trunc)
    sum(acs5tract_sae_counts$age_0to5_count)
  }
  
  ### Run comparison with ACS1 aggregates
  agg_comp_model <- 
    compare_sae_agg(sae_in        = sae_vc_out,
                    sae_count_var = "vc_model_count_trunc",
                    sae_vul_cat   = "vc_value",
                    agg_data      = sae_input_data,
                    agg_vul_cat   = vul_vars)
  
  agg_comp_fh <- 
    compare_sae_agg(sae_in        = sae_vc_out,
                    sae_count_var = "vc_fh_count_trunc",
                    sae_vul_cat   = "vc_value",
                    agg_data      = sae_input_data,
                    agg_vul_cat   = vul_vars)
  
  ### Collect SAE output
  out <- 
    list(direct_out = direct_vc_out,
         sae_out    = sae_vc_out,
         agg_comp_model = agg_comp_model,
         agg_comp_fh    = agg_comp_fh)
  
  # visualize the comparison between the direct and the SAE output
  if (viz_comp) {
    viz <- 
      ggplot(sae_vc_out,
             aes(x = share_direct,
                 y = share_fh,
                 color = vc_value)) +
      geom_point() +
      geom_abline(intercept = 0,
                  slope = 1) +
      facet_wrap(~ vc_value) +
      coord_fixed() + 
      theme_minimal()
    out[["comp_viz"]] <- viz
  }
  
  return(out)
}
```
